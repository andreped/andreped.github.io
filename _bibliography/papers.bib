---
---

@inbook{pedersen2022AIchapter,
abbr={Book chapter},
author = {Pedersen, André and Reinertsen, Ingerid and Janssen, Emiel and Valla, Marit},
year = {2022},
month = {01},
pages = {365-375},
title = {Artificial Intelligence in Studies of Malignant Tumours. In Book: Biomarkers of the Tumor Microenvironment},
bookTitle={Biomarkers of the Tumor Microenvironment},
isbn = {978-3-030-98949-1},
doi = {doi.org/10.1007/978-3-030-98950-7_21},
url={https://doi.org/10.1007/978-3-030-98950-7_21},
pages={365--375},
publisher={Springer International Publishing},
abstract={With the introduction of digital pathology and artificial intelligence (AI)-based methods, we may be facing a new era in cancer diagnostics and prognostication. AI can assist pathologists in labour-intensive tasks and potentially discover new features currently not detected and characterized in routine diagnostics. As entire digital histopathological sections can be included in the analysis, AI can be used both to study the epithelial component of a tumour and the microenvironment. Most state-of-the-art AI approaches used for image analysis utilize multi-step pipelines. AI-based methods have shown promising results in a wide range of clinically relevant tasks. It is, however, important to be aware of some challenges and limitations, such as the lack of generalizability of AI-based models, and the importance of understanding the reason behind a conclusion.},
address={Cham},
rgbvalue={15},
bibtex_show={true}
}

@ARTICLE{10.3389/fneur.2022.932219,
abbr={Fron.Neu},
AUTHOR={Bouget, David and Pedersen, André and Jakola, Asgeir S. and Kavouridis, Vasileios and Emblem, Kyrre E. and Eijgelaar, Roelant S. and Kommers, Ivar and Ardon, Hilko and Barkhof, Frederik and Bello, Lorenzo and Berger, Mitchel S. and Conti Nibali, Marco and Furtner, Julia and Hervey-Jumper, Shawn and Idema, Albert J. S. and Kiesel, Barbara and Kloet, Alfred and Mandonnet, Emmanuel and Müller, Domenique M. J. and Robe, Pierre A. and Rossi, Marco and Sciortino, Tommaso and Van den Brink, Wimar A. and Wagemakers, Michiel and Widhalm, Georg and Witte, Marnix G. and Zwinderman, Aeilko H. and De Witt Hamer, Philip C. and Solheim, Ole and Reinertsen, Ingerid},
TITLE={Preoperative Brain Tumor Imaging: Models and Software for Segmentation and Standardized Reporting},
JOURNAL={Frontiers in Neurology},
VOLUME={13},
YEAR={2022},
URL={https://www.frontiersin.org/articles/10.3389/fneur.2022.932219},
DOI={10.3389/fneur.2022.932219}, 
ISSN={1664-2295},
code={https://github.com/dbouget/Raidionics},
pdf={https://www.frontiersin.org/articles/10.3389/fneur.2022.932219/full},
ABSTRACT={For patients suffering from brain tumor, prognosis estimation and treatment decisions are made by a multidisciplinary team based on a set of preoperative MR scans. Currently, the lack of standardized and automatic methods for tumor detection and generation of clinical reports, incorporating a wide range of tumor characteristics, represents a major hurdle. In this study, we investigate the most occurring brain tumor types: glioblastomas, lower grade gliomas, meningiomas, and metastases, through four cohorts of up to 4,000 patients. Tumor segmentation models were trained using the AGU-Net architecture with different preprocessing steps and protocols. Segmentation performances were assessed in-depth using a wide-range of voxel and patient-wise metrics covering volume, distance, and probabilistic aspects. Finally, two software solutions have been developed, enabling an easy use of the trained models and standardized generation of clinical reports: Raidionics and Raidionics-Slicer. Segmentation performances were quite homogeneous across the four different brain tumor types, with an average true positive Dice ranging between 80 and 90%, patient-wise recall between 88 and 98%, and patient-wise precision around 95%. In conjunction to Dice, the identified most relevant other metrics were the relative absolute volume difference, the variation of information, and the Hausdorff, Mahalanobis, and object average symmetric surface distances. With our Raidionics software, running on a desktop computer with CPU support, tumor segmentation can be performed in 16–54 s depending on the dimensions of the MRI volume. For the generation of a standardized clinical report, including the tumor segmentation and features computation, 5–15 min are necessary. All trained models have been made open-access together with the source code for both software solutions and validation metrics computation. In the future, a method to convert results from a set of metrics into a final single score would be highly desirable for easier ranking across trained models. In addition, an automatic classification of the brain tumor type would be necessary to replace manual user input. Finally, the inclusion of post-operative segmentation in both software solutions will be key for generating complete post-operative standardized clinical reports.},
rgbvalue={14},
bibtex_show={true}
}

@article{fredriksen2021teacherstudentlung,
abbr={PLOS ONE},
doi = {10.1371/journal.pone.0266147},
bibtex_show={true},
author = {Fredriksen, Vemund AND Sevle, Svein Ole M. AND Pedersen, André AND Langø, Thomas AND Kiss, Gabriel AND Lindseth, Frank},
journal = {PLOS ONE},
publisher = {Public Library of Science},
title = {Teacher-student approach for lung tumor segmentation from mixed-supervised datasets},
year = {2022},
month = {04},
volume = {17},
url = {https://doi.org/10.1371/journal.pone.0266147},
pages = {1-14},
code={https://github.com/VemundFredriksen/LungTumorMask},
pdf={https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0266147},
abstract = {Purpose Cancer is among the leading causes of death in the developed world, and lung cancer is the most lethal type. Early detection is crucial for better prognosis, but can be resource intensive to achieve. Automating tasks such as lung tumor localization and segmentation in radiological images can free valuable time for radiologists and other clinical personnel. Convolutional neural networks may be suited for such tasks, but require substantial amounts of labeled data to train. Obtaining labeled data is a challenge, especially in the medical domain.   Methods This paper investigates the use of a teacher-student design to utilize datasets with different types of supervision to train an automatic model performing pulmonary tumor segmentation on computed tomography images. The framework consists of two models: the student that performs end-to-end automatic tumor segmentation and the teacher that supplies the student additional pseudo-annotated data during training.   Results Using only a small proportion of semantically labeled data and a large number of bounding box annotated data, we achieved competitive performance using a teacher-student design. Models trained on larger amounts of semantic annotations did not perform better than those trained on teacher-annotated data. Our model trained on a small number of semantically labeled data achieved a mean dice similarity coefficient of 71.0 on the MSD Lung dataset.   Conclusions Our results demonstrate the potential of utilizing teacher-student designs to reduce the annotation load, as less supervised annotation schemes may be performed, without any real degradation in segmentation accuracy.},
number = {4},
rgvalue={1}
}

@article{bouget2021mediastinal,
abbr = {Comp.M.Bio},
bibtex_show = {true},
author = {David Bouget and André Pedersen and Johanna Vanel and Haakon O. Leira and Thomas Langø},
title = {Mediastinal lymph nodes segmentation using 3D convolutional neural network ensembles and anatomical priors guiding},
journal = {Computer Methods in Biomechanics and Biomedical Engineering: Imaging \& Visualization},
volume = {0},
number = {0},
pages = {1-15},
year  = {2022},
month = {03},
pdf={https://www.tandfonline.com/doi/full/10.1080/21681163.2022.2043778},
code={https://github.com/dbouget/ct_mediastinal_structures_segmentation},
publisher = {Taylor & Francis},
doi = {10.1080/21681163.2022.2043778},
URL = {https://doi.org/10.1080/21681163.2022.2043778},
eprint = {https://doi.org/10.1080/21681163.2022.2043778},
abstract={As lung cancer evolves, the presence of potentially malignant lymph nodes must be assessed to properly estimate disease progression and select the best treatment strategy. A method for accurate and automatic segmentation is hence decisive for quantitatively describing lymph nodes. In this study, the use of 3D convolutional neural networks, either through slab-wise schemes or the leveraging of downsampled entire volumes, is investigated. As lymph nodes have similar attenuation values to nearby anatomical structures, we use the knowledge of other organs as prior information to guide the segmentation. To assess the performances, a 5-fold cross-validation strategy was followed over a dataset of 120 contrast-enhanced CT volumes. For the 1178 lymph nodes with a short-axis diameter ≥10 mm, our best-performing approach reached a patient-wise recall of 92%, a false positive per patient ratio of 5 and a segmentation overlap of 80.5%. Fusing a slab-wise and a full volume approach within an ensemble scheme generated the best performances. The anatomical priors guiding strategy is promising, yet a larger set than four organs appears needed to generate an optimal benefit. A larger dataset is also mandatory given the wide range of expressions a lymph node can exhibit (i.e. shape, location and attenuation).},
rgvalue = {10}
}

@article{yan2021sepsis,
abbr={IEEE-B},
bibtex_show={true},
title={Preliminary Processing and Analysis of an Adverse Event Dataset for Detecting Sepsis-Related Events},
author={Yan, Melissa and Høvik, Lise Husby and Pedersen, André and Gustad, Lise Tuset and Nytrø, Øystein},
journal={2021 IEEE International Conference on Bioinformatics and Biomedicine},
year={2021},
month={12},
code={https://github.com/andreped/adverse-events},
abstract={Adverse event (AE) reports contain notes detailing procedural and guideline deviations, and unwanted incidents that can bring harm to patients. Available datasets mainly focus on vigilance or post-market surveillance of adverse drug reactions or
medical device failures. The lack of clinical-related AE datasets makes it challenging to study healthcare-related AEs. AEs affect 10% of hospitalized patients, and almost half are preventable. Having an AE dataset can assist in identifying possible patient
safety interventions and performing quality surveillance to lower AE rates. The free-text notes can provide insight into the cause of incidents and lead to better patient care. The objective of this study is to introduce a Norwegian AE dataset and present preliminary processing and analysis for sepsis-related events, specifically peripheral intravenous catheter-related bloodstream infections. Therefore, the methods focus on performing a domain analysis to prepare and better understand the data through screening, generating synthetic free-text notes, and annotating
notes.},
rgvalue={2}
}

@article{pedersen2021hybrid,
abbr={arXiv},
bibtex_show={true},
title={Hybrid guiding: A multi-resolution refinement approach for semantic segmentation of gigapixel histopathological images}, 
author={André Pedersen and Erik Smistad and Tor V. Rise and Vibeke G. Dale and Henrik S. Pettersen and Tor-Arne S. Nordmo and David Bouget and Ingerid Reinertsen and Marit Valla},
journal={arXiv},
year={2021},
eprint={2112.03455},
archivePrefix={arXiv},
primaryClass={eess.IV},
pdf={https://arxiv.org/pdf/2112.03455.pdf},
code={https://github.com/andreped/H2G-Net},
poster={https://github.com/andreped/andreped.github.io/blob/master/assets/posters/H2G_Net_HMN_conference_spring_2022.pdf},
abstract={Histopathological cancer diagnostics has become more complex, and the increasing number of biopsies is a challenge for most pathology laboratories. Thus, development of automatic methods for evaluation of histopathological cancer sections would be of value. In this study, we used 624 whole slide images (WSIs) of breast cancer from a Norwegian cohort. We propose a cascaded convolutional neural network design, called H2G-Net, for semantic segmentation of gigapixel histopathological images. The design involves a detection stage using a patch-wise method, and a refinement stage using a convolutional autoencoder. To validate the design, we conducted an ablation study to assess the impact of selected components in the pipeline on tumour segmentation. Guiding segmentation, using hierarchical sampling and deep heatmap refinement, proved to be beneficial when segmenting the histopathological images. We found a significant improvement when using a refinement network for postprocessing the generated tumour segmentation heatmaps. The overall best design achieved a Dice score of 0.933 on an independent test set of 90 WSIs. The design outperformed single-resolution approaches, such as cluster-guided, patch-wise high-resolution classification using MobileNetV2 (0.872) and a low-resolution U-Net (0.874). In addition, segmentation on a representative x400 WSI took ~58 seconds, using only the CPU. The findings demonstrate the potential of utilizing a refinement network to improve patch-wise predictions. The solution is efficient and does not require overlapping patch inference or ensembling. Furthermore, we showed that deep neural networks can be trained using a random sampling scheme that balances on multiple different labels simultaneously, without the need of storing patches on disk. Future work should involve more efficient patch generation and sampling, as well as improved clustering.},
rgvalue={3}
}

@article{pettersen2021codefree,
abbr={Fron.Med},
bibtex_show={true},
title={Code-Free Development and Deployment of Deep Segmentation Models for Digital Pathology}, 
author={Henrik S. Pettersen and Ilya Belevich and Elin S. Røyset and Melanie R. Simpson and Erik Smistad and Eija Jokitalo and Ingerid Reinertsen and Ingunn Bakke and André Pedersen},
year={2022},
month={01},
url={https://www.frontiersin.org/article/10.3389/fmed.2021.816281},
journal={Frontiers in Medicine},
volume={8},
issn={2296-858X},
pdf={https://www.frontiersin.org/articles/10.3389/fmed.2021.816281/full},
code={https://github.com/andreped/NoCodeSeg},
video={https://www.youtube.com/watch?v=9dTfUwnL6zY},
abstract={Application of deep learning on histopathological whole slide images (WSIs) holds promise of improving diagnostic efficiency and reproducibility but is largely dependent on the ability to write computer code or purchase commercial solutions. We present a code-free pipeline utilizing free-to-use, open-source software (QuPath, DeepMIB, and FastPathology) for creating and deploying deep learning-based segmentation models for computational pathology. We demonstrate the pipeline on a use case of separating epithelium from stroma in colonic mucosa. A dataset of 251 annotated WSIs, comprising 140 hematoxylin-eosin (HE)-stained and 111 CD3 immunostained colon biopsy WSIs, were developed through active learning using the pipeline. On a hold-out test set of 36 HE and 21 CD3-stained WSIs a mean intersection over union score of 95.5 and 95.3% was achieved on epithelium segmentation. We demonstrate pathologist-level segmentation accuracy and clinical acceptable runtime performance and show that pathologists without programming experience can create near state-of-the-art segmentation solutions for histopathological WSIs using only free-to-use software. The study further demonstrates the strength of open-source solutions in its ability to create generalizable, open pipelines, of which trained models and predictions can seamlessly be exported in open formats and thereby used in external solutions. All scripts, trained models, a video tutorial, and the full dataset of 251 WSIs with ~31 k epithelium annotations are made openly available at https://github.com/andreped/NoCodeSeg to accelerate research in the field.},
rgvalue={4}
}

@article{bouget2021glioma,
abbr={Cancers},
bibtex_show={true},
author = {Bouget, David and Eijgelaar, Roelant and Pedersen, André and Kommers, Ivar and Ardon, Hilko and Barkhof, Frederik and Bello, Lorenzo and Berger, Mitchel and Conti Nibali, Marco and Furtner, Julia and Fyllingen, Even and Hervey-Jumper, Shawn and Idema, Albert and Kiesel, Barbara and Kloet, Alfred and Mandonnet, Emmanuel and Müller, Domenique and Robe, Pierre and Rossi, Marco and Solheim, Ole},
year = {2021},
month = {09},
pages = {},
title = {Glioblastoma Surgery Imaging-Reporting and Data System: Validation and Performance of the Automated Segmentation Task},
volume = {13},
journal = {Cancers},
doi = {10.3390/cancers13184674},
pdf = {https://www.mdpi.com/2072-6694/13/18/4674/htm},
code = {https://github.com/SINTEFMedtek/GSI-RADS},
abstract = {For patients with presumed glioblastoma, essential tumor characteristics are determined from preoperative MR images to optimize the treatment strategy. This procedure is time-consuming and subjective, if performed by crude eyeballing or manually. The standardized GSI-RADS aims to provide neurosurgeons with automatic tumor segmentations to extract tumor features rapidly and objectively. In this study, we improved automatic tumor segmentation and compared the agreement with manual raters, describe the technical details of the different components of GSI-RADS, and determined their speed. Two recent neural network architectures were considered for the segmentation task: nnU-Net and AGU-Net. Two preprocessing schemes were introduced to investigate the tradeoff between performance and processing speed. A summarized description of the tumor feature extraction and standardized reporting process is included. The trained architectures for automatic segmentation and the code for computing the standardized report are distributed as open-source and as open-access software. Validation studies were performed on a dataset of 1594 gadolinium-enhanced T1-weighted MRI volumes from 13 hospitals and 293 T1-weighted MRI volumes from the BraTS challenge. The glioblastoma tumor core segmentation reached a Dice score slightly below 90%, a patientwise F1-score close to 99%, and a 95th percentile Hausdorff distance slightly below 4.0 mm on average with either architecture and the heavy preprocessing scheme. A patient MRI volume can be segmented in less than one minute, and a standardized report can be generated in up to five minutes. The proposed GSI-RADS software showed robust performance on a large collection of MRI volumes from various hospitals and generated results within a reasonable runtime.},
rgvalue={5}
}

@article{bouget2021attention,
abbr={Fron.Rad},
bibtex_show={true},
author = {Bouget, David and Pedersen, André and Hosainey, Sayied and Solheim, Ole and Reinertsen, Ingerid},
year = {2021},
month = {09},
pages = {711514},
title = {Meningioma Segmentation in T1-Weighted MRI Leveraging Global Context and Attention Mechanisms},
volume = {1},
journal = {Frontiers in Radiology},
doi = {10.3389/fradi.2021.711514},
pdf = {https://www.frontiersin.org/articles/10.3389/fradi.2021.711514/full},
code = {https://github.com/dbouget/mri_brain_tumor_segmentation},
abstract={Purpose: Meningiomas are the most common type of primary brain tumor, accounting for ~30% of all brain tumors. A substantial number of these tumors are never surgically removed but rather monitored over time. Automatic and precise meningioma segmentation is, therefore, beneficial to enable reliable growth estimation and patient-specific treatment planning. Methods: In this study, we propose the inclusion of attention mechanisms on top of a U-Net architecture used as backbone: (i) Attention-gated U-Net (AGUNet) and (ii) Dual Attention U-Net (DAUNet), using a three-dimensional (3D) magnetic resonance imaging (MRI) volume as input. Attention has the potential to leverage the global context and identify features' relationships across the entire volume. To limit spatial resolution degradation and loss of detail inherent to encoder–decoder architectures, we studied the impact of multi-scale input and deep supervision components. The proposed architectures are trainable end-to-end and each concept can be seamlessly disabled for ablation studies. Results: The validation studies were performed using a five-fold cross-validation over 600 T1-weighted MRI volumes from St. Olavs Hospital, Trondheim University Hospital, Norway. Models were evaluated based on segmentation, detection, and speed performances, and results are reported patient-wise after averaging across all folds. For the best-performing architecture, an average Dice score of 81.6% was reached for an F1-score of 95.6%. With an almost perfect precision of 98%, meningiomas smaller than 3 ml were occasionally missed hence reaching an overall recall of 93%. Conclusion: Leveraging global context from a 3D MRI volume provided the best performances, even if the native volume resolution could not be processed directly due to current GPU memory limitations. Overall, near-perfect detection was achieved for meningiomas larger than 3 ml, which is relevant for clinical use. In the future, the use of multi-scale designs and refinement networks should be further investigated. A larger number of cases with meningiomas below 3 ml might also be needed to improve the performance for the smallest tumors.},
rgvalue={6}
}

@article{kommers2021gsirads,
abbr={Cancers},
bibtex_show={true},
author = {Kommers, Ivar and Bouget, David and Pedersen, André and Eijgelaar, Roelant and Ardon, Hilko and Barkhof, Frederik and Bello, Lorenzo and Berger, Mitchel and Conti Nibali, Marco and Furtner, Julia and Fyllingen, Even and Hervey-Jumper, Shawn and Idema, Albert and Kiesel, Barbara and Kloet, Alfred and Mandonnet, Emmanuel and Müller, Domenique and Robe, Pierre and Rossi, Marco and De Witt Hamer, Philip},
year = {2021},
month = {06},
pages = {2854},
title = {Glioblastoma Surgery Imaging—Reporting and Data System: Standardized Reporting of Tumor Volume, Location, and Resectability Based on Automated Segmentations},
volume = {13},
journal = {Cancers},
doi = {10.3390/cancers13122854},
pdf={https://www.mdpi.com/2072-6694/13/12/2854/htm},
code={https://github.com/SINTEFMedtek/GSI-RADS},
abstract={Treatment decisions for patients with presumed glioblastoma are based on tumor characteristics available from a preoperative MR scan. Tumor characteristics, including volume, location, and resectability, are often estimated or manually delineated. This process is time consuming and subjective. Hence, comparison across cohorts, trials, or registries are subject to assessment bias. In this study, we propose a standardized Glioblastoma Surgery Imaging Reporting and Data System (GSI-RADS) based on an automated method of tumor segmentation that provides standard reports on tumor features that are potentially relevant for glioblastoma surgery. As clinical validation, we determine the agreement in extracted tumor features between the automated method and the current standard of manual segmentations from routine clinical MR scans before treatment. In an observational consecutive cohort of 1596 adult patients with a first time surgery of a glioblastoma from 13 institutions, we segmented gadolinium-enhanced tumor parts both by a human rater and by an automated algorithm. Tumor features were extracted from segmentations of both methods and compared to assess differences, concordance, and equivalence. The laterality, contralateral infiltration, and the laterality indices were in excellent agreement. The native and normalized tumor volumes had excellent agreement, consistency, and equivalence. Multifocality, but not the number of foci, had good agreement and equivalence. The location profiles of cortical and subcortical structures were in excellent agreement. The expected residual tumor volumes and resectability indices had excellent agreement, consistency, and equivalence. Tumor probability maps were in good agreement. In conclusion, automated segmentations are in excellent agreement with manual segmentations and practically equivalent regarding tumor features that are potentially relevant for neurosurgical purposes. Standard GSI-RADS reports can be generated by open access software.},
rgvalue={7}
}

@article{pedersen2021fastpathology,
abbr={IEEE-A},
bibtex_show={true},
author = {Pedersen, André and Valla, Marit and Bofin, Anna and Pérez de Frutos, Javier and Reinertsen, Ingerid and Smistad, Erik},
year = {2021},
month = {04},
pages = {1-1},
title = {FastPathology: An Open-Source Platform for Deep Learning-Based Research and Decision Support in Digital Pathology},
volume = {PP},
journal = {IEEE Access},
doi = {10.1109/ACCESS.2021.3072231},
selected={true},
pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9399433},
code={https://github.com/AICAN-Research/FAST-Pathology},
video={https://www.youtube.com/watch?v=1s7jU6T7S3U},
abstract={Deep convolutional neural networks (CNNs) are the current state-of-the-art for digital analysis of histopathological images. The large size of whole-slide microscopy images (WSIs) requires advanced memory handling to read, display and process these images. There are several open-source platforms for working with WSIs, but few support deployment of CNN models. These applications use third-party solutions for inference, making them less user-friendly and unsuitable for high-performance image analysis. To make deployment of CNNs user-friendly and feasible on low-end machines, we have developed a new platform, FastPathology, using the FAST framework and C++. It minimizes memory usage for reading and processing WSIs, deployment of CNN models, and real-time interactive visualization of results. Runtime experiments were conducted on four different use cases, using different architectures, inference engines, hardware configurations and operating systems. Memory usage for reading, visualizing, zooming and panning a WSI were measured, using FastPathology and three existing platforms. FastPathology performed similarly in terms of memory to the other C++-based application, while using considerably less than the two Java-based platforms. The choice of neural network model, inference engine, hardware and processors influenced runtime considerably. Thus, FastPathology includes all steps needed for efficient visualization and processing of WSIs in a single application, including inference of CNNs with real-time display of the results. Source code, binary releases, video demonstrations and test data can be found online on GitHub at https://github.com/SINTEFMedtek/FAST-Pathology/.},
rgvalue={8}
}

@article{bouget2021meningioma,
abbr={JMI},
bibtex_show={true},
author = {Bouget, David and Pedersen, André and Hosainey, Sayied and Vanel, Johanna and Solheim, Ole and Reinertsen, Ingerid},
year = {2021},
month = {03},
pages = {1-16},
title = {Fast meningioma segmentation in T1-weighted magnetic resonance imaging volumes using a lightweight 3D deep learning architecture},
volume = {8},
journal = {Journal of Medical Imaging},
doi = {10.1117/1.JMI.8.2.024002},
pdf={https://arxiv.org/pdf/2010.07002.pdf},
abstract={Purpose: Automatic and consistent meningioma segmentation in T1-weighted magnetic resonance (MR) imaging volumes and corresponding volumetric assessment is of use for diagnosis, treatment planning, and tumor growth evaluation. We optimized the segmentation and processing speed performances using a large number of both surgically treated meningiomas and untreated meningiomas followed at the outpatient clinic. Approach: We studied two different three-dimensional (3D) neural network architectures: (i) a simple encoder-decoder similar to a 3D U-Net, and (ii) a lightweight multi-scale architecture [Pulmonary Lobe Segmentation Network (PLS-Net)]. In addition, we studied the impact of different training schemes. For the validation studies, we used 698 T1-weighted MR volumes from St. Olav University Hospital, Trondheim, Norway. The models were evaluated in terms of detection accuracy, segmentation accuracy, and training/inference speed. Results: While both architectures reached a similar Dice score of 70% on average, the PLS-Net was more accurate with an F1-score of up to 88%. The highest accuracy was achieved for the largest meningiomas. Speed-wise, the PLS-Net architecture tended to converge in about 50 h while 130 h were necessary for U-Net. Inference with PLS-Net takes less than a second on GPU and about 15 s on CPU. Conclusions: Overall, with the use of mixed precision training, it was possible to train competitive segmentation models in a relatively short amount of time using the lightweight PLS-Net architecture. In the future, the focus should be brought toward the segmentation of small meningiomas (<2 ml) to improve clinical relevance for automatic and early diagnosis and speed of growth estimates.},
rgvalue={9}
}

@article{Snipstad2021bubblecan,
abbr={UMB},
bibtex_show={true},
author = {Snipstad, Sofie and Mørch, Yrr and Sulheim, Einar and Aslund, Andreas and Davies, Catharina and Hansen, Rune and Berg, Sigrid and Pedersen, André},
year = {2021},
month = {02},
pages = {},
title = {Sonopermeation Enhances Uptake and Therapeutic Effect of Free and Encapsulated Cabazitaxel},
volume = {47},
journal = {Ultrasound in Medicine & Biology},
doi = {10.1016/j.ultrasmedbio.2020.12.026},
pdf={https://www.umbjournal.org/article/S0301-5629(21)00004-1/fulltext},
abstract={Delivery of drugs and nanomedicines to tumors is often heterogeneous and insufficient and, thus, of limited efficacy. Microbubbles in combination with ultrasound have been found to improve delivery to tumors, enhancing accumulation and penetration. We used a subcutaneous prostate cancer xenograft model in mice to investigate the effect of free and nanoparticle-encapsulated cabazitaxel in combination with ultrasound and microbubbles with a lipid shell or a shell of nanoparticles. Sonopermeation reduced tumor growth and prolonged survival (26%-100%), whether the free drug was co-injected with lipid-shelled microbubbles or the nanoformulation was co-injected with lipid-shelled or nanoparticle-shelled microbubbles. Coherently with the improved therapeutic response, we found enhanced uptake of nanoparticles directly after ultrasound treatment that lasted several weeks (2.3x-15.8x increase). Neither cavitation dose nor total accumulation of nanoparticles could explain the variation within treatment groups, emphasizing the need for a better understanding of the tumor biology and mechanisms involved in ultrasound-mediated treatment.},
rgvalue={11}
}

@article{smistad2019fast,
abbr={IEEE-A},
bibtex_show={true},
author = {Smistad, Erik and Østvik, Andreas and Pedersen, André},
year = {2019},
month = {09},
pages = {1-1},
title = {High Performance Neural Network Inference, Streaming, and Visualization of Medical Images Using FAST},
volume = {PP},
journal = {IEEE Access},
doi = {10.1109/ACCESS.2019.2942441},
pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8844665},
code={https://github.com/smistad/FAST},
video={https://www.youtube.com/watch?v=iuevRnZMDgg},
abstract={Deep convolutional neural networks have quickly become the standard for medical image analysis. Although there are many frameworks focusing on training neural networks, there are few that focus on high performance inference and visualization of medical images. Neural network inference requires an inference engine (IE), and there are currently several IEs available including Intel’s OpenVINO, NVIDIA’s TensorRT, and Google’s TensorFlow which supports multiple backends, including NVIDIA’s cuDNN, AMD’s ROCm and Intel’s MKL-DNN. These IEs only work on specific processors and have completely different application programming interfaces (APIs). In this paper, we presents methods for extending FAST, an open-source high performance framework for medical imaging, to use any IE with a common programming interface. Thereby making it easier for users to deploy and test their neural networks on different processors. This article provides an overview of current IEs and how they can be combined with existing software such as FAST. The methods are demonstrated and evaluated on three performance demanding medical use cases: real-time ultrasound image segmentation, computed tomography (CT) volume segmentation, and patch-wise classification of whole slide microscopy images. Runtime performance was measured on the three use cases with several different IEs and processors. This revealed that the choice of IE and processor can affect performance of medical neural network image analysis considerably. In the most extreme case of processing 171 ultrasound frames, the difference between the fastest and slowest configuration were half a second vs. 24 seconds. For volume processing, using the CPU or the GPU, showed a difference of 2 vs. 53 seconds, and for processing an whole slide microscopy image, the difference was 81 seconds vs. almost 16 minutes. Source code, binary releases and test data can be found online on GitHub at https://github.com/smistad/FAST/.},
rgvalue={12}
}
